{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sv_wide_df = pd.read_pickle(open(\"../data/processed/20220301_straatvinken_abt_complete_df.pkl\", \"rb\"))\n",
    "sv_wide_df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "\n",
    "We need to adjust for missing values, which starts with knowing which columns contain them.\n",
    "\n",
    "Luckily there's only few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sv_wide_df.sample(n=10)[[\"lat\", \"long\"]].to_csv(\"../data/raw/infer_coordinates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_wide_df.isnull().sum()[sv_wide_df.isnull().sum().index[sv_wide_df.isnull().sum() > 0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can neglect the img_* and lat_right, long_right, segm_distance and pano_id being NA because they won't be used. Which leaves only the number of cars / households and the verharding (road pavement type?).\n",
    "\n",
    "For the latter we'll take the majority class (verh=1, lbl=weg met vaste verharding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sv_wide_df.verhlbl.value_counts())\n",
    "print(sv_wide_df.verh.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_wide_df.verh = sv_wide_df.verh.fillna(\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For number of households and affiliated metrics, we'll use median imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_wide_df.nhh = sv_wide_df.nhh.fillna(sv_wide_df.nhh.median())\n",
    "sv_wide_df.ncars = sv_wide_df.ncars.fillna(sv_wide_df.ncars.median())\n",
    "sv_wide_df.ncars_hh = sv_wide_df.ncars / sv_wide_df.nhh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we dealt with all missing values, we make a selection of the columns to use for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_seq_items = 1000\n",
    "display(sv_wide_df.columns)\n",
    "pd.options.display.max_seq_items = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be predicted columns\n",
    "Y_s = ['bike', 'bus', 'car', 'truck', 'van', 'walk']\n",
    "# columns to be one-hot-encoded (for categorical variables)\n",
    "one_hot_cols = ['verh', 'morf', 'wegcat']\n",
    "# columns that are to be left the way they are (no normalization)\n",
    "num_pred_remain_cols = ['is_natw']\n",
    "num_pred_segm_cols = ['segm_wall', 'segm_building',\n",
    "       'segm_sky', 'segm_tree', 'segm_road', 'segm_grass', 'segm_sidewalk',\n",
    "       'segm_car', 'segm_fence', 'segm_signboard', 'segm_pole', 'segm_person',\n",
    "       'segm_plant', 'segm_stairs', 'segm_bridge', 'segm_streetlight',\n",
    "       'segm_earth', 'segm_water', 'segm_rock', 'segm_box', 'segm_sand',\n",
    "       'segm_path', 'segm_bench', 'segm_house', 'segm_van', 'segm_minibike',\n",
    "       'segm_ashcan', 'segm_pot', 'segm_mountain', 'segm_door', 'segm_trade',\n",
    "       'segm_field', 'segm_floor', 'segm_dirt', 'segm_flower', 'segm_truck',\n",
    "       'segm_railing', 'segm_boat', 'segm_conveyer', 'segm_windowpane',\n",
    "       'segm_ceiling', 'segm_stairway', 'segm_bus', 'segm_bicycle',\n",
    "       'segm_chair', 'segm_cabinet', 'segm_table', 'segm_awning',\n",
    "       'segm_bannister', 'segm_escalator', 'segm_bed']\n",
    "num_pred_minmax_cols = ['bedi', 'pode', 'nhh', 'ncars', 'ncars_hh', 'ms_pop', \n",
    "    'acc', 'acc_death', 'acc_death30', 'acc_mort',\n",
    "    'acc_ser', 'acc_sly',\n",
    "    'nrijstr', 'wb', 'wegcat_H_50', 'wegcat_L2_50', 'wegcat_PII-1_50', 'wegcat_S2_50',\n",
    "       'wegcat_PII-2_50', 'wegcat_S_50', 'wegcat_L1_50', 'wegcat_L_50',\n",
    "       'wegcat_L3_50', 'wegcat_PI_50', 'wegcat_PII-4_50', 'wegcat_S3_50',\n",
    "       'wegcat_S1_50', 'wegcat_S4_50', 'wegcat_PII_50', 'wegcat_Htot_50',\n",
    "       'wegcat_Ptot_50', 'wegcat_Stot_50', 'wegcat_Ltot_50',\n",
    "       'morf_130_50', 'morf_120_50', 'morf_101_50',\n",
    "       'morf_102_50', 'morf_103_50', 'morf_104_50', 'morf_105_50',\n",
    "       'morf_106_50', 'morf_107_50', 'morf_108_50', 'morf_109_50',\n",
    "       'morf_110_50', 'morf_111_50', 'morf_112_50', 'morf_113_50',\n",
    "       'morf_114_50', 'morf_116_50', 'morf_-8_50', 'morf_125_50',\n",
    "       'wegcat_H_100', 'wegcat_L2_100',\n",
    "       'wegcat_PII-1_100', 'wegcat_S2_100', 'wegcat_PII-2_100', 'wegcat_S_100',\n",
    "       'wegcat_L1_100', 'wegcat_L_100', 'wegcat_L3_100', 'wegcat_PI_100',\n",
    "       'wegcat_PII-4_100', 'wegcat_S3_100', 'wegcat_S1_100', 'wegcat_S4_100',\n",
    "       'wegcat_PII_100', 'wegcat_Htot_100', 'wegcat_Ptot_100',\n",
    "       'wegcat_Stot_100', 'wegcat_Ltot_100', \n",
    "       'morf_130_100', 'morf_120_100', 'morf_101_100', 'morf_102_100',\n",
    "       'morf_103_100', 'morf_104_100', 'morf_105_100', 'morf_106_100',\n",
    "       'morf_107_100', 'morf_108_100', 'morf_109_100', 'morf_110_100',\n",
    "       'morf_111_100', 'morf_112_100', 'morf_113_100', 'morf_114_100',\n",
    "       'morf_116_100', 'morf_-8_100', 'morf_125_100', \n",
    "       'wegcat_H_150', 'wegcat_L2_150', 'wegcat_PII-1_150', 'wegcat_S2_150',\n",
    "       'wegcat_PII-2_150', 'wegcat_S_150', 'wegcat_L1_150', 'wegcat_L_150',\n",
    "       'wegcat_L3_150', 'wegcat_PI_150', 'wegcat_PII-4_150', 'wegcat_S3_150',\n",
    "       'wegcat_S1_150', 'wegcat_S4_150', 'wegcat_PII_150', 'wegcat_Htot_150',\n",
    "       'wegcat_Ptot_150', 'wegcat_Stot_150', 'wegcat_Ltot_150',\n",
    "       'morf_130_150', 'morf_120_150',\n",
    "       'morf_101_150', 'morf_102_150', 'morf_103_150', 'morf_104_150',\n",
    "       'morf_105_150', 'morf_106_150', 'morf_107_150', 'morf_108_150',\n",
    "       'morf_109_150', 'morf_110_150', 'morf_111_150', 'morf_112_150',\n",
    "       'morf_113_150', 'morf_114_150', 'morf_116_150', 'morf_-8_150',\n",
    "       'morf_125_150', 'wegcat_H_250',\n",
    "       'wegcat_L2_250', 'wegcat_PII-1_250', 'wegcat_S2_250',\n",
    "       'wegcat_PII-2_250', 'wegcat_S_250', 'wegcat_L1_250', 'wegcat_L_250',\n",
    "       'wegcat_L3_250', 'wegcat_PI_250', 'wegcat_PII-4_250', 'wegcat_S3_250',\n",
    "       'wegcat_S1_250', 'wegcat_S4_250', 'wegcat_PII_250', 'wegcat_Htot_250',\n",
    "       'wegcat_Ptot_250', 'wegcat_Stot_250', 'wegcat_Ltot_250',\n",
    "       'morf_130_250', 'morf_120_250',\n",
    "       'morf_101_250', 'morf_102_250', 'morf_103_250', 'morf_104_250',\n",
    "       'morf_105_250', 'morf_106_250', 'morf_107_250', 'morf_108_250',\n",
    "       'morf_109_250', 'morf_110_250', 'morf_111_250', 'morf_112_250',\n",
    "       'morf_113_250', 'morf_114_250', 'morf_116_250', 'morf_-8_250',\n",
    "       'morf_125_250', 'wegcat_H_500',\n",
    "       'wegcat_L2_500', 'wegcat_PII-1_500', 'wegcat_S2_500',\n",
    "       'wegcat_PII-2_500', 'wegcat_S_500', 'wegcat_L1_500', 'wegcat_L_500',\n",
    "       'wegcat_L3_500', 'wegcat_PI_500', 'wegcat_PII-4_500', 'wegcat_S3_500',\n",
    "       'wegcat_S1_500', 'wegcat_S4_500', 'wegcat_PII_500', 'wegcat_Htot_500',\n",
    "       'wegcat_Ptot_500', 'wegcat_Stot_500', 'wegcat_Ltot_500',\n",
    "       'morf_130_500', 'morf_120_500',\n",
    "       'morf_101_500', 'morf_102_500', 'morf_103_500', 'morf_104_500',\n",
    "       'morf_105_500', 'morf_106_500', 'morf_107_500', 'morf_108_500',\n",
    "       'morf_109_500', 'morf_110_500', 'morf_111_500', 'morf_112_500',\n",
    "       'morf_113_500', 'morf_114_500', 'morf_116_500', 'morf_-8_500',\n",
    "       'morf_125_500', 'wegcat_H_1000',\n",
    "       'wegcat_L2_1000', 'wegcat_PII-1_1000', 'wegcat_S2_1000',\n",
    "       'wegcat_PII-2_1000', 'wegcat_S_1000', 'wegcat_L1_1000', 'wegcat_L_1000',\n",
    "       'wegcat_L3_1000', 'wegcat_PI_1000', 'wegcat_PII-4_1000',\n",
    "       'wegcat_S3_1000', 'wegcat_S1_1000', 'wegcat_S4_1000', 'wegcat_PII_1000',\n",
    "       'wegcat_Htot_1000', 'wegcat_Ptot_1000', 'wegcat_Stot_1000',\n",
    "       'wegcat_Ltot_1000', 'morf_130_1000',\n",
    "       'morf_120_1000', 'morf_101_1000', 'morf_102_1000', 'morf_103_1000',\n",
    "       'morf_104_1000', 'morf_105_1000', 'morf_106_1000', 'morf_107_1000',\n",
    "       'morf_108_1000', 'morf_109_1000', 'morf_110_1000', 'morf_111_1000',\n",
    "       'morf_112_1000', 'morf_113_1000', 'morf_114_1000', 'morf_116_1000',\n",
    "       'morf_-8_1000', 'morf_125_1000', \n",
    "       'wegcat_H_2000', 'wegcat_L2_2000', 'wegcat_PII-1_2000',\n",
    "       'wegcat_S2_2000', 'wegcat_PII-2_2000', 'wegcat_S_2000',\n",
    "       'wegcat_L1_2000', 'wegcat_L_2000', 'wegcat_L3_2000', 'wegcat_PI_2000',\n",
    "       'wegcat_PII-4_2000', 'wegcat_S3_2000', 'wegcat_S1_2000',\n",
    "       'wegcat_S4_2000', 'wegcat_PII_2000', 'wegcat_Htot_2000',\n",
    "       'wegcat_Ptot_2000', 'wegcat_Stot_2000', 'wegcat_Ltot_2000',\n",
    "       'morf_130_2000', 'morf_120_2000',\n",
    "       'morf_101_2000', 'morf_102_2000', 'morf_103_2000', 'morf_104_2000',\n",
    "       'morf_105_2000', 'morf_106_2000', 'morf_107_2000', 'morf_108_2000',\n",
    "       'morf_109_2000', 'morf_110_2000', 'morf_111_2000', 'morf_112_2000',\n",
    "       'morf_113_2000', 'morf_114_2000', 'morf_116_2000', 'morf_-8_2000',\n",
    "       'morf_125_2000']\n",
    "print(f\"Y_s {len(Y_s)}\")\n",
    "print(f\"one_hot_cols {len(one_hot_cols)}\")\n",
    "print(f\"num_pred_remain_cols {len(num_pred_remain_cols)}\")\n",
    "print(f\"num_pred_segm_cols {len(num_pred_segm_cols)}\")\n",
    "print(f\"num_pred_minmax_cols {len(num_pred_minmax_cols)}\")\n",
    "print(f\"sum pred cols {len(one_hot_cols) + len(num_pred_remain_cols) + len(num_pred_segm_cols) + + len(num_pred_minmax_cols)}\")\n",
    "pred_cols = one_hot_cols.copy()\n",
    "pred_cols.extend(num_pred_remain_cols)\n",
    "pred_cols.extend(num_pred_segm_cols)\n",
    "pred_cols.extend(num_pred_minmax_cols)\n",
    "print(\"actual # pred cols\", len(pred_cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting columns to YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "COLUMN_CONFIG_FILE = \"../src/aicityflowsstraatvinken/columns_2022_1.3.yaml\"\n",
    "config= {\n",
    "    \"iteration\": \"2022 1.3\",\n",
    "    \"date_created\": date.today().isoformat(),\n",
    "    \"Y_s\": Y_s, \n",
    "    \"one_hot_cols\": one_hot_cols, \n",
    "    \"num_pred_remain_cols\": num_pred_remain_cols,\n",
    "    \"num_pred_minmax_cols\": num_pred_minmax_cols,\n",
    "    \"num_pred_segm_cols\": num_pred_segm_cols\n",
    "}\n",
    "with open(COLUMN_CONFIG_FILE, 'w') as f:\n",
    "    config = yaml.dump(config, stream=f,\n",
    "                       default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the columns selected, we can make a split of the dataset in a train and test set:.\n",
    "\n",
    "Some important considerations:\n",
    "* when the dataset consists of the whole of Flanders (at time of writing this is not the case), being able to pick evenly from all provinces could be interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline #, make_union\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor, XGBRFRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import pickle\n",
    "from datetime import date\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sv_wide_df[pred_cols], \n",
    "    sv_wide_df[Y_s], \n",
    "    test_size=.2, \n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    #stratify=sf_wide_df[\"\"]\n",
    ")\n",
    "\n",
    "pickle.dump((X_train, X_test, y_train, y_test), open(f\"../data/training/{date.today().isoformat().replace('-', '')}_train_test_data.pkl\", 'wb'))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline performance\n",
    "\n",
    "For the sake of having a frame of reference to which to compare our model's accuracy, we can calculate some baseline performance metrics:\n",
    "* the mean value\n",
    "* the median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = y_test.shape[0]\n",
    "maea = mean_absolute_error(y_test['walk'], [y_train[\"walk\"].mean()] * n_samples)\n",
    "rmsea = np.sqrt(mean_squared_error(y_test['walk'], [y_train[\"walk\"].mean()]*y_test.shape[0]))\n",
    "print(f\"Average value MAE {maea:8.4f} RMSE {rmsea:8.4f}\")\n",
    "maem = mean_absolute_error(y_test['walk'], [y_train[\"walk\"].median()]*n_samples)\n",
    "rmsem = np.sqrt(mean_squared_error(y_test['walk'], [y_train[\"walk\"].median()]*y_test.shape[0]))\n",
    "print(f\"Median value  MAE {maem:8.4f} RMSE {rmsem:8.4f}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat this exercise for any percentile and plot the lowest error we could achieve this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "q = np.linspace(0, 1, 101)\n",
    "mae = [mean_absolute_error(y_test[\"walk\"], [quant]*n_samples) for quant in np.quantile(y_train['walk'], q)]\n",
    "rmse = [np.sqrt(mean_squared_error(y_test[\"walk\"], [quant]*n_samples)) for quant in np.quantile(y_train['walk'], q)]\n",
    "quantdf = pd.DataFrame({\"quantile\": q, \"mae\": mae, \"rmse\": rmse})\n",
    "\n",
    "fig, ax=plt.subplots(figsize=(10, 5))\n",
    "ax.scatter(quantdf[\"quantile\"], quantdf.mae, label=\"mae\")\n",
    "ax.scatter(quantdf[\"quantile\"], quantdf.rmse, label=\"rmse\")\n",
    "ax.set_xlabel(\"quantile of training data\")\n",
    "ax.set_ylabel(\"error\")\n",
    "ax.set_ylim(0, 100)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f\"min mae  {quantdf.mae.min():8.4f} \\nmin rmse {quantdf.rmse.min():8.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "quantdf[quantdf.mae == quantdf.mae.min()]\n",
    "quantdf[quantdf.rmse == quantdf.rmse.min()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/opt/anaconda3/envs/straatvinken/bin/pip install pytorch-tabnet\n",
    "#!/opt/anaconda3/envs/straatvinken/bin/pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Columns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, names=None):\n",
    "        self.names = names\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_categories = list(sv_wide_df[one_hot_cols].apply(lambda x: list(set(x)), axis=0).values)\n",
    "prefix = np.hstack([[one_hot_cols[ix]] * (len(cat_list)-1) for ix, cat_list in enumerate(one_hot_categories)]) \n",
    "suffix =  np.hstack([cat_list[1:] for cat_list in one_hot_categories]).astype(str)\n",
    "one_hot_col_names = [f\"{prefix}_{suffix}\" for prefix, suffix in zip(prefix, suffix)]\n",
    "one_hot_col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pred_minmax = num_pred_minmax_cols.copy()\n",
    "num_pred_minmax.extend(num_pred_segm_cols)\n",
    "_pipeline_def = (\"features\", FeatureUnion([\n",
    "    ('ohe', make_pipeline(\n",
    "        Columns(names=one_hot_cols),\n",
    "        OneHotEncoder(sparse=False, drop=\"first\", categories=one_hot_categories, handle_unknown=\"error\"))),\n",
    "    ('mima', make_pipeline(\n",
    "        Columns(names=num_pred_minmax),\n",
    "        MinMaxScaler())),\n",
    "    ('keep', make_pipeline(Columns(names=num_pred_remain_cols)))\n",
    "]))\n",
    "data_transformation_pipe = Pipeline(\n",
    "    [_pipeline_def]\n",
    ")\n",
    "\n",
    "_X_trans = data_transformation_pipe.fit_transform(X_train)\n",
    "\n",
    "cols = one_hot_col_names.copy()\n",
    "cols.extend(num_pred_minmax)\n",
    "cols.extend(num_pred_remain_cols)\n",
    "_X_train_df = pd.DataFrame(_X_trans, columns=cols)\n",
    "_X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, fixed\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sel=widgets.SelectMultiple(\n",
    "    options=cols, \n",
    "    value=cols[:10], \n",
    "    description=\"columns\",\n",
    "    #rows=5,\n",
    ")\n",
    "\n",
    "def show_corr(corr_data, rows):\n",
    "    general_cols=[\"bike\", \"walk\", \"bus\",\"car\", \"truck\", \"van\"]\n",
    "    wegcat_cols = rows\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    cmap = sns.color_palette(\"Spectral\", as_cmap=True)\n",
    "    sns.heatmap(corr_data.loc[rows, general_cols], annot=True, cmap=cmap, center=0, vmin=-1, vmax=1)\n",
    "    ax.xaxis.tick_top()\n",
    "    plt.title(f\"Correlation plot\")\n",
    "    plt.show()\n",
    "\n",
    "@interact(cols=sel, plottype=[\"corr\", \"hist\"])\n",
    "def show_plots(cols, plottype):\n",
    "    frag_df = pd.concat([_X_train_df[list(cols)], y_train], axis=1)\n",
    "    if plottype==\"hist\":\n",
    "        fig, ax = plt.subplots(figsize=(12, 15))\n",
    "        frag_df[list(cols)].hist(ax=ax)\n",
    "        plt.show()\n",
    "    else:\n",
    "        corr = frag_df.corr()\n",
    "        show_corr(corr, rows=cols)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Elastic Net using Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/opt/anaconda3/envs/straatvinken/bin/pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        _pipeline_def,\n",
    "        ('est', ElasticNetCV(cv=10, random_state=42))\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#_gs = GridSearchCV(pipe, param_grid=[params], scoring='neg_mean_absolute_error', cv=10)\n",
    "result = pipe.fit(X_train, y_train['walk']) \n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Net fit results\n",
    "\n",
    "Let's analyse the results of the model fit. \n",
    "The code below shows\n",
    "* selected hyperparameter values (alpha and l1_ratio)\n",
    "* beta coefficients (some of which are zeroes due to l1 penalty shrinkage)\n",
    "* the intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.corrcoef()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"alpha {pipe.named_steps['est'].alpha_:.4f}: The amount of penalization choosen by cross validation\")\n",
    "\n",
    "print(f\"l1_ratio {pipe.named_steps['est'].l1_ratio_:.4f}: The compromise between l1 and l2 penalization choosen by cross validation\")\n",
    "\n",
    "coeffs = {col: coeff for col, coeff in zip(cols, pipe.named_steps['est'].coef_)}\n",
    "\n",
    "ZERO_THRESHOLD = 0.0001\n",
    "zeroes = len([val for val in coeffs.values() if abs(val) < ZERO_THRESHOLD])\n",
    "coeffs = dict(sorted(coeffs.items(), key=lambda item: abs(item[1]), reverse=True))\n",
    "\n",
    "print(f\"coefficients: parameter vector (zeroes: {zeroes}/{len(cols)})\")\n",
    "MAX_ITEMS=40\n",
    "[print(f'   {col:<16}: {coeff:8.4f}') for ix, (col, coeff) in enumerate(coeffs.items()) if abs(coeff) > ZERO_THRESHOLD and ix < MAX_ITEMS]\n",
    "print(\"    ...\")\n",
    "print(f\"intercept {pipe.named_steps['est'].intercept_:.4f}\")\n",
    "#print(f\"mse_path {pipe.named_steps['est'].mse_path_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the 10 largest (absolute) coefficients are assigned to \n",
    "   * ms_pop (pop density in square grid): 43.8277\n",
    "   * segm_trade (segmentation data: %trade name, brand name): 32.0203\n",
    "   * ncars_hh (# cars per household in statistical sector): -31.1926\n",
    "   * bedi (building density): 23.3342\n",
    "   * segm_sky (segmentation data: % sky): -23.2176\n",
    "   * ncars (# cars in statistical sector): -20.2385\n",
    "   * morf_110_50 (# roads of morphology type 110 aka \"ventweg\" within 50m): 18.6296\n",
    "   * segm_person (segmentation data: % persons): 16.5171\n",
    "   * segm_awning (segmentation data: % awning, sunshade, sunblind): 16.4896\n",
    "   * nhh (# households in statistical sector): 15.5749\n",
    "   \n",
    "Let's see how well the model did, in terms of MAE and RMSE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the mean absolute error in a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(result.predict(X_train) - y_train['walk']).hist(bins=25)\n",
    "\n",
    "plt.title(\"Prediction errors on training data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(result.predict(X_test) - y_test['walk']).hist(bins=25)\n",
    "plt.title(\"Prediction errors on test data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's for a minute look at the predictions for which the error was very large (> 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"ElasticNet_abs_error_walk\"] = result.predict(X_test) - y_test['walk']\n",
    "X_test[\"ElasticNet_large_error_walk\"] = abs(X_test[\"ElasticNet_abs_error_walk\"]) >= 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[X_test[\"ElasticNet_large_error_walk\"]].sort_values(by=\"ElasticNet_abs_error_walk\", ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_cb = Pipeline(\n",
    "    [\n",
    "        _pipeline_def\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_cb.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = CatBoostRegressor()\n",
    "\n",
    "grid = {'learning_rate': [0.03, 0.1],\n",
    "        'depth': [4, 6, 10],\n",
    "        'l2_leaf_reg': [1, 3, 5, 7, 9]}\n",
    "\n",
    "grid_search_result = model.grid_search(grid,\n",
    "                                       X=pipe_cb.fit_transform(X_train),\n",
    "                                       y=y_train[\"walk\"],\n",
    "                                       plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "print(\"mean abs error\", mean_absolute_error(model.predict(pipe_cb.fit_transform(X_test)), y_test[\"walk\"]))\n",
    "print(\"mean squared error\", np.sqrt(mean_squared_error(model.predict(pipe_cb.fit_transform(X_test)), y_test[\"walk\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quentin_gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "quentin_df = pd.read_csv(\"../data/raw/infer_coordinates_quentin_large.csv\")\n",
    "quentin_df\n",
    "quentin_gpd = gpd.GeoDataFrame(quentin_df, geometry=gpd.geoseries.from_wkt(quentin_df.representative_point.values, crs=4326))\n",
    "quentin_df\n",
    "display(quentin_gpd)\n",
    "quentin_gpd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "quentin_df[[\"long\", \"lat\"]] = quentin_df.apply(lambda p: (p.geometry.x, p.geometry.y), axis=1, result_type=\"expand\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quentin_df = quentin_df.drop(columns=[\"lon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "quentin_df.to_csv(\"../data/raw/infer_coordinates_quentin_large.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "df = pickle.load(open(\"../data/processed/20220303-infer_output.pkl\", \"rb\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quentin_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quentin_df = quentin_df.loc[~quentin_df.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 (straatvinken)",
   "language": "python",
   "name": "straatvinken"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
